{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JHpF-BfJUDph"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLVbPaHIUZgq",
        "outputId": "430cb9dc-6dec-45cb-9be3-182f2cb59ab9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***TAKE INPUT***"
      ],
      "metadata": {
        "id": "SlJsLdwFUD9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load 4D data\n",
        "test_Normal = np.load('/content/drive/My Drive/data/test_Normal_128.npy')\n",
        "train_virus = np.load('/content/drive/My Drive/data/train_Virus_128.npy')\n",
        "train_Normal = np.load('/content/drive/My Drive/data/train_Normal_128.npy')\n",
        "test_virus = np.load('/content/drive/My Drive/data/test_Virus_128.npy')\n",
        "torch.manual_seed(101)"
      ],
      "metadata": {
        "id": "MfgP4iEb-xc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c82a4e-8f0a-4db7-f8ac-7a9109be5418"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f12f4206a30>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Data Preprocessing***"
      ],
      "metadata": {
        "id": "NHFcD0OOUNwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Train Data\n",
        "train_normal = torch.from_numpy(train_Normal)\n",
        "train_virus = torch.from_numpy(train_virus)\n",
        "train_normal_label = torch.zeros(len(train_normal))\n",
        "train_virus_label = torch.ones(len(train_virus))\n",
        "train_data = torch.cat((train_normal, train_virus), axis=0)\n",
        "train_label = torch.cat((train_normal_label, train_virus_label), axis=0)\n",
        "print(train_data.shape)\n",
        "train_data = torch.reshape(train_data, (2686,1,128,128))\n",
        "\n",
        "\n",
        "#Test Data\n",
        "test_normal = test_Normal.astype(np.float32)\n",
        "test_normal = torch.from_numpy(test_normal)\n",
        "test_virus = torch.from_numpy(test_virus)\n",
        "test_normal_label = torch.zeros(len(test_normal))\n",
        "test_virus_label = torch.ones(len(test_virus))\n",
        "test_data = torch.cat((test_normal, test_virus), axis=0)\n",
        "test_label = torch.cat((test_normal_label, test_virus_label), axis=0)\n",
        "print(test_data.shape)\n",
        "test_data = torch.reshape(test_data,(382,1,128,128))"
      ],
      "metadata": {
        "id": "ZE4SwoyW2Ycw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c676d7ab-ce23-48cb-942e-9cadc76fa279"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2686, 128, 128, 1])\n",
            "torch.Size([382, 128, 128, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traindata = [train_data[i] for i in range(len(train_data))]\n",
        "train = torch.stack([d[0] for d in traindata], dim=0)\n",
        "testdata = [test_data[i] for i in range(len(test_data))]\n",
        "test = torch.stack([d[0] for d in testdata], dim=0)       \n",
        "\n",
        "print(len(traindata))\n",
        "print(test.shape)\n",
        "print(train.shape)"
      ],
      "metadata": {
        "id": "1Lu4ib-q3VoW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aee4214-75a3-4915-b855-6a53df224c5e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2686\n",
            "torch.Size([382, 128, 128])\n",
            "torch.Size([2686, 128, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = train_label  \n",
        "test_y = test_label"
      ],
      "metadata": {
        "id": "oKsQi1YiDVmN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Data shuffling***"
      ],
      "metadata": {
        "id": "Rb3Qaib6UXOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset= (train,train_y) , \n",
        "                                           shuffle=True)   \n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset= (test,test_y), \n",
        "                                         shuffle=False)"
      ],
      "metadata": {
        "id": "WHKO2pz2BPzR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Implementing CNN***"
      ],
      "metadata": {
        "id": "vKY801HEVe3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.cnn_layer_1 = nn.Conv2d(in_channels=1, out_channels=16,kernel_size=5, stride=1, padding=2)\n",
        "        self.cnn_layer_2 = nn.Conv2d(in_channels=16, out_channels=32,kernel_size=5, stride=1, padding=2)\n",
        "        self.cnn_layer_3 = nn.Conv2d(in_channels=32 ,out_channels=64,kernel_size=5, stride=1, padding=2)\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.maxpool = nn.MaxPool2d(2,2)\n",
        "        \n",
        "        self.linear_layer_1 = nn.Linear(64*64*16, 512) \n",
        "        self.linear_layer_2 = nn.Linear(512, 256)\n",
        "        self.linear_layer_3 = nn.Linear(256, 128)\n",
        "        self.linear_layer_4= nn.Linear(128, 2)   \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.cnn_layer_1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x) \n",
        "        x = self.sigmoid(x)\n",
        "        \n",
        "        x = self.cnn_layer_2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x=self.sigmoid(x)\n",
        "\n",
        "        x = self.cnn_layer_3(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)      \n",
        "        x=self.sigmoid(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        \n",
        "        x = self.linear_layer_1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        \n",
        "        x = self.linear_layer_2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        x = self.linear_layer_3(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.linear_layer_4(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        logits = self.sigmoid(x)\n",
        "        return x\n",
        "     "
      ],
      "metadata": {
        "id": "qd_JADlrEBi8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = NeuralNetwork(128*128)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "MF37YJv1F4y5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0e9d3b-051b-4f51-d003-708a3f8fdaec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (cnn_layer_1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (cnn_layer_2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (cnn_layer_3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (linear_layer_1): Linear(in_features=65536, out_features=512, bias=True)\n",
              "  (linear_layer_2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (linear_layer_3): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (linear_layer_4): Linear(in_features=128, out_features=2, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (sigmoid): Sigmoid()\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Implementing Loss function and Optimizer***"
      ],
      "metadata": {
        "id": "zLs2XGHvVouj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.0005\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "     "
      ],
      "metadata": {
        "id": "xU6y5DT2F-eA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Train The Model***"
      ],
      "metadata": {
        "id": "1v8DLbVjWDuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainModel(model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    batch = 200\n",
        "    loss = 0\n",
        "    \n",
        "    for i in range(train.shape[0]):\n",
        "      x, y = torch.reshape(train[i],(1,1,128,128)), torch.tensor([train_y[i]], dtype=torch.float)\n",
        "      label=torch.zeros([1,2], dtype=torch.float32)\n",
        "      label[0,int(y.item())]=1\n",
        "      #Compute prediction error\n",
        "      predicted= model(x)\n",
        "      loss += criterion(predicted, label)\n",
        "      \n",
        "      if i>0 and (i+1)%batch == 0:\n",
        "          #Backpropagation\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          loss = 0\n",
        "    print('Training Loss:{}'.format(loss.item()))\n",
        "    print(\"<====================================================================================>\")"
      ],
      "metadata": {
        "id": "lGc3EoB8GBsg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Test The Model***"
      ],
      "metadata": {
        "id": "t6Zbj8YqWOUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "   \n",
        "    size = test.shape[0]\n",
        "    correct=0\n",
        "    loss = 0\n",
        "    iter=240\n",
        "    total =382\n",
        "    trainModel(model, criterion, optimizer)\n",
        "    \n",
        "    for i in range(iter):\n",
        "         x, y = torch.reshape(test[i],(1,1,128,128)), torch.tensor([test_y[i]], dtype=torch.float)\n",
        "         label=torch.zeros([1,2], dtype=torch.float32)\n",
        "         label[0,int(y.item())]=1\n",
        "         pred = model(x)  \n",
        "         _, predicted = torch.max(pred, 1)\n",
        "          # Total correct predictions\n",
        "         correct += (predicted == int(y)).sum()\n",
        "         loss += criterion(pred, label)\n",
        "      \n",
        "         loss /= size\n",
        "         accuracy = 100 * correct.item() / total\n",
        "\n",
        "         # Print Loss and accuracy\n",
        "         print('Iteration: {}. || Loss: {}. ||  Accuracy: {}'.format(i+1,loss.item(), accuracy))\n",
        "         print(\"<====================================================================================>\")\n",
        "   "
      ],
      "metadata": {
        "id": "Sol6Y3emGFEA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "359e5a33-894c-4d3a-e804-0fb4971fd88f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss:59.610618591308594\n",
            "<====================================================================================>\n",
            "Iteration: 1. || Loss: 0.0018145213834941387. ||  Accuracy: 0.2617801047120419\n",
            "<====================================================================================>\n",
            "Iteration: 2. || Loss: 0.0018192714778706431. ||  Accuracy: 0.5235602094240838\n",
            "<====================================================================================>\n",
            "Iteration: 3. || Loss: 0.0018192838178947568. ||  Accuracy: 0.7853403141361257\n",
            "<====================================================================================>\n",
            "Iteration: 4. || Loss: 0.0018192840507254004. ||  Accuracy: 1.0471204188481675\n",
            "<====================================================================================>\n",
            "Iteration: 5. || Loss: 0.0018192840507254004. ||  Accuracy: 1.3089005235602094\n",
            "<====================================================================================>\n",
            "Iteration: 6. || Loss: 0.0018192840507254004. ||  Accuracy: 1.5706806282722514\n",
            "<====================================================================================>\n",
            "Iteration: 7. || Loss: 0.0018192840507254004. ||  Accuracy: 1.8324607329842932\n",
            "<====================================================================================>\n",
            "Iteration: 8. || Loss: 0.0018192840507254004. ||  Accuracy: 2.094240837696335\n",
            "<====================================================================================>\n",
            "Iteration: 9. || Loss: 0.0018192840507254004. ||  Accuracy: 2.356020942408377\n",
            "<====================================================================================>\n",
            "Iteration: 10. || Loss: 0.0018192840507254004. ||  Accuracy: 2.6178010471204187\n",
            "<====================================================================================>\n",
            "Iteration: 11. || Loss: 0.0018192840507254004. ||  Accuracy: 2.8795811518324608\n",
            "<====================================================================================>\n",
            "Iteration: 12. || Loss: 0.0018192840507254004. ||  Accuracy: 3.141361256544503\n",
            "<====================================================================================>\n",
            "Iteration: 13. || Loss: 0.0018192840507254004. ||  Accuracy: 3.4031413612565444\n",
            "<====================================================================================>\n",
            "Iteration: 14. || Loss: 0.0018192840507254004. ||  Accuracy: 3.6649214659685865\n",
            "<====================================================================================>\n",
            "Iteration: 15. || Loss: 0.0018192840507254004. ||  Accuracy: 3.926701570680628\n",
            "<====================================================================================>\n",
            "Iteration: 16. || Loss: 0.0018192840507254004. ||  Accuracy: 4.18848167539267\n",
            "<====================================================================================>\n",
            "Iteration: 17. || Loss: 0.0018192840507254004. ||  Accuracy: 4.450261780104712\n",
            "<====================================================================================>\n",
            "Iteration: 18. || Loss: 0.0018192840507254004. ||  Accuracy: 4.712041884816754\n",
            "<====================================================================================>\n",
            "Iteration: 19. || Loss: 0.0018192840507254004. ||  Accuracy: 4.973821989528796\n",
            "<====================================================================================>\n",
            "Iteration: 20. || Loss: 0.0018192840507254004. ||  Accuracy: 5.2356020942408374\n",
            "<====================================================================================>\n",
            "Iteration: 21. || Loss: 0.0018192840507254004. ||  Accuracy: 5.49738219895288\n",
            "<====================================================================================>\n",
            "Iteration: 22. || Loss: 0.0018192840507254004. ||  Accuracy: 5.7591623036649215\n",
            "<====================================================================================>\n",
            "Iteration: 23. || Loss: 0.0018192840507254004. ||  Accuracy: 6.020942408376963\n",
            "<====================================================================================>\n",
            "Iteration: 24. || Loss: 0.0018192840507254004. ||  Accuracy: 6.282722513089006\n",
            "<====================================================================================>\n",
            "Iteration: 25. || Loss: 0.0018192840507254004. ||  Accuracy: 6.544502617801047\n",
            "<====================================================================================>\n",
            "Iteration: 26. || Loss: 0.0018192840507254004. ||  Accuracy: 6.806282722513089\n",
            "<====================================================================================>\n",
            "Iteration: 27. || Loss: 0.0018192840507254004. ||  Accuracy: 7.0680628272251305\n",
            "<====================================================================================>\n",
            "Iteration: 28. || Loss: 0.0018192840507254004. ||  Accuracy: 7.329842931937173\n",
            "<====================================================================================>\n",
            "Iteration: 29. || Loss: 0.0018192840507254004. ||  Accuracy: 7.591623036649215\n",
            "<====================================================================================>\n",
            "Iteration: 30. || Loss: 0.0018192840507254004. ||  Accuracy: 7.853403141361256\n",
            "<====================================================================================>\n",
            "Iteration: 31. || Loss: 0.0018192840507254004. ||  Accuracy: 8.115183246073299\n",
            "<====================================================================================>\n",
            "Iteration: 32. || Loss: 0.0018192840507254004. ||  Accuracy: 8.37696335078534\n",
            "<====================================================================================>\n",
            "Iteration: 33. || Loss: 0.0018192840507254004. ||  Accuracy: 8.638743455497382\n",
            "<====================================================================================>\n",
            "Iteration: 34. || Loss: 0.0018192840507254004. ||  Accuracy: 8.900523560209423\n",
            "<====================================================================================>\n",
            "Iteration: 35. || Loss: 0.0018192840507254004. ||  Accuracy: 9.162303664921465\n",
            "<====================================================================================>\n",
            "Iteration: 36. || Loss: 0.0018192840507254004. ||  Accuracy: 9.424083769633508\n",
            "<====================================================================================>\n",
            "Iteration: 37. || Loss: 0.0018192840507254004. ||  Accuracy: 9.68586387434555\n",
            "<====================================================================================>\n",
            "Iteration: 38. || Loss: 0.0018192840507254004. ||  Accuracy: 9.947643979057592\n",
            "<====================================================================================>\n",
            "Iteration: 39. || Loss: 0.0018192840507254004. ||  Accuracy: 10.209424083769633\n",
            "<====================================================================================>\n",
            "Iteration: 40. || Loss: 0.0018192840507254004. ||  Accuracy: 10.471204188481675\n",
            "<====================================================================================>\n",
            "Iteration: 41. || Loss: 0.0018192840507254004. ||  Accuracy: 10.732984293193716\n",
            "<====================================================================================>\n",
            "Iteration: 42. || Loss: 0.0018192840507254004. ||  Accuracy: 10.99476439790576\n",
            "<====================================================================================>\n",
            "Iteration: 43. || Loss: 0.0018192840507254004. ||  Accuracy: 11.256544502617801\n",
            "<====================================================================================>\n",
            "Iteration: 44. || Loss: 0.0018192840507254004. ||  Accuracy: 11.518324607329843\n",
            "<====================================================================================>\n",
            "Iteration: 45. || Loss: 0.0018192840507254004. ||  Accuracy: 11.780104712041885\n",
            "<====================================================================================>\n",
            "Iteration: 46. || Loss: 0.0018192840507254004. ||  Accuracy: 12.041884816753926\n",
            "<====================================================================================>\n",
            "Iteration: 47. || Loss: 0.0018192840507254004. ||  Accuracy: 12.303664921465968\n",
            "<====================================================================================>\n",
            "Iteration: 48. || Loss: 0.0018192840507254004. ||  Accuracy: 12.565445026178011\n",
            "<====================================================================================>\n",
            "Iteration: 49. || Loss: 0.0018192840507254004. ||  Accuracy: 12.827225130890053\n",
            "<====================================================================================>\n",
            "Iteration: 50. || Loss: 0.0018192840507254004. ||  Accuracy: 13.089005235602095\n",
            "<====================================================================================>\n",
            "Iteration: 51. || Loss: 0.0018192840507254004. ||  Accuracy: 13.350785340314136\n",
            "<====================================================================================>\n",
            "Iteration: 52. || Loss: 0.0018192840507254004. ||  Accuracy: 13.612565445026178\n",
            "<====================================================================================>\n",
            "Iteration: 53. || Loss: 0.0018192840507254004. ||  Accuracy: 13.87434554973822\n",
            "<====================================================================================>\n",
            "Iteration: 54. || Loss: 0.0018192840507254004. ||  Accuracy: 14.136125654450261\n",
            "<====================================================================================>\n",
            "Iteration: 55. || Loss: 0.0018192840507254004. ||  Accuracy: 14.397905759162304\n",
            "<====================================================================================>\n",
            "Iteration: 56. || Loss: 0.0018192840507254004. ||  Accuracy: 14.659685863874346\n",
            "<====================================================================================>\n",
            "Iteration: 57. || Loss: 0.0018192840507254004. ||  Accuracy: 14.921465968586388\n",
            "<====================================================================================>\n",
            "Iteration: 58. || Loss: 0.0018192840507254004. ||  Accuracy: 15.18324607329843\n",
            "<====================================================================================>\n",
            "Iteration: 59. || Loss: 0.0018192840507254004. ||  Accuracy: 15.44502617801047\n",
            "<====================================================================================>\n",
            "Iteration: 60. || Loss: 0.0018192840507254004. ||  Accuracy: 15.706806282722512\n",
            "<====================================================================================>\n",
            "Iteration: 61. || Loss: 0.0018192840507254004. ||  Accuracy: 15.968586387434556\n",
            "<====================================================================================>\n",
            "Iteration: 62. || Loss: 0.0018192840507254004. ||  Accuracy: 16.230366492146597\n",
            "<====================================================================================>\n",
            "Iteration: 63. || Loss: 0.0018192840507254004. ||  Accuracy: 16.49214659685864\n",
            "<====================================================================================>\n",
            "Iteration: 64. || Loss: 0.0018192840507254004. ||  Accuracy: 16.75392670157068\n",
            "<====================================================================================>\n",
            "Iteration: 65. || Loss: 0.0018192840507254004. ||  Accuracy: 17.015706806282722\n",
            "<====================================================================================>\n",
            "Iteration: 66. || Loss: 0.0018192840507254004. ||  Accuracy: 17.277486910994764\n",
            "<====================================================================================>\n",
            "Iteration: 67. || Loss: 0.0018192840507254004. ||  Accuracy: 17.539267015706805\n",
            "<====================================================================================>\n",
            "Iteration: 68. || Loss: 0.0018192840507254004. ||  Accuracy: 17.801047120418847\n",
            "<====================================================================================>\n",
            "Iteration: 69. || Loss: 0.0018192840507254004. ||  Accuracy: 18.06282722513089\n",
            "<====================================================================================>\n",
            "Iteration: 70. || Loss: 0.0018192840507254004. ||  Accuracy: 18.32460732984293\n",
            "<====================================================================================>\n",
            "Iteration: 71. || Loss: 0.0018192840507254004. ||  Accuracy: 18.586387434554975\n",
            "<====================================================================================>\n",
            "Iteration: 72. || Loss: 0.0018192840507254004. ||  Accuracy: 18.848167539267017\n",
            "<====================================================================================>\n",
            "Iteration: 73. || Loss: 0.0018192840507254004. ||  Accuracy: 19.10994764397906\n",
            "<====================================================================================>\n",
            "Iteration: 74. || Loss: 0.0018192840507254004. ||  Accuracy: 19.3717277486911\n",
            "<====================================================================================>\n",
            "Iteration: 75. || Loss: 0.0018192840507254004. ||  Accuracy: 19.63350785340314\n",
            "<====================================================================================>\n",
            "Iteration: 76. || Loss: 0.0018192840507254004. ||  Accuracy: 19.895287958115183\n",
            "<====================================================================================>\n",
            "Iteration: 77. || Loss: 0.0018192840507254004. ||  Accuracy: 20.157068062827225\n",
            "<====================================================================================>\n",
            "Iteration: 78. || Loss: 0.0018192840507254004. ||  Accuracy: 20.418848167539267\n",
            "<====================================================================================>\n",
            "Iteration: 79. || Loss: 0.0018192840507254004. ||  Accuracy: 20.680628272251308\n",
            "<====================================================================================>\n",
            "Iteration: 80. || Loss: 0.0018192840507254004. ||  Accuracy: 20.94240837696335\n",
            "<====================================================================================>\n",
            "Iteration: 81. || Loss: 0.0018192840507254004. ||  Accuracy: 21.20418848167539\n",
            "<====================================================================================>\n",
            "Iteration: 82. || Loss: 0.0018192840507254004. ||  Accuracy: 21.465968586387433\n",
            "<====================================================================================>\n",
            "Iteration: 83. || Loss: 0.0018192840507254004. ||  Accuracy: 21.727748691099478\n",
            "<====================================================================================>\n",
            "Iteration: 84. || Loss: 0.0018192840507254004. ||  Accuracy: 21.98952879581152\n",
            "<====================================================================================>\n",
            "Iteration: 85. || Loss: 0.0018192840507254004. ||  Accuracy: 22.25130890052356\n",
            "<====================================================================================>\n",
            "Iteration: 86. || Loss: 0.0018192840507254004. ||  Accuracy: 22.513089005235603\n",
            "<====================================================================================>\n",
            "Iteration: 87. || Loss: 0.0018192840507254004. ||  Accuracy: 22.774869109947645\n",
            "<====================================================================================>\n",
            "Iteration: 88. || Loss: 0.0018192840507254004. ||  Accuracy: 23.036649214659686\n",
            "<====================================================================================>\n",
            "Iteration: 89. || Loss: 0.0018192840507254004. ||  Accuracy: 23.298429319371728\n",
            "<====================================================================================>\n",
            "Iteration: 90. || Loss: 0.0018192840507254004. ||  Accuracy: 23.56020942408377\n",
            "<====================================================================================>\n",
            "Iteration: 91. || Loss: 0.0018192840507254004. ||  Accuracy: 23.82198952879581\n",
            "<====================================================================================>\n",
            "Iteration: 92. || Loss: 0.0018192840507254004. ||  Accuracy: 24.083769633507853\n",
            "<====================================================================================>\n",
            "Iteration: 93. || Loss: 0.0018192840507254004. ||  Accuracy: 24.345549738219894\n",
            "<====================================================================================>\n",
            "Iteration: 94. || Loss: 0.0018192840507254004. ||  Accuracy: 24.607329842931936\n",
            "<====================================================================================>\n",
            "Iteration: 95. || Loss: 0.0018192840507254004. ||  Accuracy: 24.869109947643977\n",
            "<====================================================================================>\n",
            "Iteration: 96. || Loss: 0.0018192840507254004. ||  Accuracy: 25.130890052356023\n",
            "<====================================================================================>\n",
            "Iteration: 97. || Loss: 0.0018192840507254004. ||  Accuracy: 25.392670157068064\n",
            "<====================================================================================>\n",
            "Iteration: 98. || Loss: 0.0018192840507254004. ||  Accuracy: 25.654450261780106\n",
            "<====================================================================================>\n",
            "Iteration: 99. || Loss: 0.0018192840507254004. ||  Accuracy: 25.916230366492147\n",
            "<====================================================================================>\n",
            "Iteration: 100. || Loss: 0.0018192840507254004. ||  Accuracy: 26.17801047120419\n",
            "<====================================================================================>\n",
            "Iteration: 101. || Loss: 0.0018192840507254004. ||  Accuracy: 26.43979057591623\n",
            "<====================================================================================>\n",
            "Iteration: 102. || Loss: 0.0018192840507254004. ||  Accuracy: 26.701570680628272\n",
            "<====================================================================================>\n",
            "Iteration: 103. || Loss: 0.0018192840507254004. ||  Accuracy: 26.963350785340314\n",
            "<====================================================================================>\n",
            "Iteration: 104. || Loss: 0.0018192840507254004. ||  Accuracy: 27.225130890052355\n",
            "<====================================================================================>\n",
            "Iteration: 105. || Loss: 0.0018192840507254004. ||  Accuracy: 27.486910994764397\n",
            "<====================================================================================>\n",
            "Iteration: 106. || Loss: 0.0018192840507254004. ||  Accuracy: 27.74869109947644\n",
            "<====================================================================================>\n",
            "Iteration: 107. || Loss: 0.0018192840507254004. ||  Accuracy: 28.01047120418848\n",
            "<====================================================================================>\n",
            "Iteration: 108. || Loss: 0.0018192840507254004. ||  Accuracy: 28.272251308900522\n",
            "<====================================================================================>\n",
            "Iteration: 109. || Loss: 0.0018192840507254004. ||  Accuracy: 28.534031413612567\n",
            "<====================================================================================>\n",
            "Iteration: 110. || Loss: 0.0018192840507254004. ||  Accuracy: 28.79581151832461\n",
            "<====================================================================================>\n",
            "Iteration: 111. || Loss: 0.0018192840507254004. ||  Accuracy: 29.05759162303665\n",
            "<====================================================================================>\n",
            "Iteration: 112. || Loss: 0.0018192840507254004. ||  Accuracy: 29.319371727748692\n",
            "<====================================================================================>\n",
            "Iteration: 113. || Loss: 0.0018192840507254004. ||  Accuracy: 29.581151832460733\n",
            "<====================================================================================>\n",
            "Iteration: 114. || Loss: 0.0018192840507254004. ||  Accuracy: 29.842931937172775\n",
            "<====================================================================================>\n",
            "Iteration: 115. || Loss: 0.0018192840507254004. ||  Accuracy: 30.104712041884817\n",
            "<====================================================================================>\n",
            "Iteration: 116. || Loss: 0.0018192840507254004. ||  Accuracy: 30.36649214659686\n",
            "<====================================================================================>\n",
            "Iteration: 117. || Loss: 0.0018192840507254004. ||  Accuracy: 30.6282722513089\n",
            "<====================================================================================>\n",
            "Iteration: 118. || Loss: 0.0018192840507254004. ||  Accuracy: 30.89005235602094\n",
            "<====================================================================================>\n",
            "Iteration: 119. || Loss: 0.0018192840507254004. ||  Accuracy: 31.151832460732983\n",
            "<====================================================================================>\n",
            "Iteration: 120. || Loss: 0.0018192840507254004. ||  Accuracy: 31.413612565445025\n",
            "<====================================================================================>\n",
            "Iteration: 121. || Loss: 0.0018192840507254004. ||  Accuracy: 31.67539267015707\n",
            "<====================================================================================>\n",
            "Iteration: 122. || Loss: 0.0018192840507254004. ||  Accuracy: 31.93717277486911\n",
            "<====================================================================================>\n",
            "Iteration: 123. || Loss: 0.0018192840507254004. ||  Accuracy: 32.19895287958115\n",
            "<====================================================================================>\n",
            "Iteration: 124. || Loss: 0.0018192840507254004. ||  Accuracy: 32.460732984293195\n",
            "<====================================================================================>\n",
            "Iteration: 125. || Loss: 0.0018192840507254004. ||  Accuracy: 32.72251308900523\n",
            "<====================================================================================>\n",
            "Iteration: 126. || Loss: 0.0018192840507254004. ||  Accuracy: 32.98429319371728\n",
            "<====================================================================================>\n",
            "Iteration: 127. || Loss: 0.0018192840507254004. ||  Accuracy: 33.246073298429316\n",
            "<====================================================================================>\n",
            "Iteration: 128. || Loss: 0.0018192840507254004. ||  Accuracy: 33.50785340314136\n",
            "<====================================================================================>\n",
            "Iteration: 129. || Loss: 0.0018192840507254004. ||  Accuracy: 33.769633507853406\n",
            "<====================================================================================>\n",
            "Iteration: 130. || Loss: 0.0018192840507254004. ||  Accuracy: 34.031413612565444\n",
            "<====================================================================================>\n",
            "Iteration: 131. || Loss: 0.0018192840507254004. ||  Accuracy: 34.29319371727749\n",
            "<====================================================================================>\n",
            "Iteration: 132. || Loss: 0.0018192840507254004. ||  Accuracy: 34.55497382198953\n",
            "<====================================================================================>\n",
            "Iteration: 133. || Loss: 0.0018192840507254004. ||  Accuracy: 34.81675392670157\n",
            "<====================================================================================>\n",
            "Iteration: 134. || Loss: 0.0018192840507254004. ||  Accuracy: 35.07853403141361\n",
            "<====================================================================================>\n",
            "Iteration: 135. || Loss: 0.0018192840507254004. ||  Accuracy: 35.340314136125656\n",
            "<====================================================================================>\n",
            "Iteration: 136. || Loss: 0.0018192840507254004. ||  Accuracy: 35.602094240837694\n",
            "<====================================================================================>\n",
            "Iteration: 137. || Loss: 0.0018192840507254004. ||  Accuracy: 35.86387434554974\n",
            "<====================================================================================>\n",
            "Iteration: 138. || Loss: 0.0018192840507254004. ||  Accuracy: 36.12565445026178\n",
            "<====================================================================================>\n",
            "Iteration: 139. || Loss: 0.0018192840507254004. ||  Accuracy: 36.38743455497382\n",
            "<====================================================================================>\n",
            "Iteration: 140. || Loss: 0.0018192840507254004. ||  Accuracy: 36.64921465968586\n",
            "<====================================================================================>\n",
            "Iteration: 141. || Loss: 0.0018192840507254004. ||  Accuracy: 36.910994764397905\n",
            "<====================================================================================>\n",
            "Iteration: 142. || Loss: 0.0018192840507254004. ||  Accuracy: 37.17277486910995\n",
            "<====================================================================================>\n",
            "Iteration: 143. || Loss: 0.0018192840507254004. ||  Accuracy: 37.43455497382199\n",
            "<====================================================================================>\n",
            "Iteration: 144. || Loss: 0.0018192840507254004. ||  Accuracy: 37.696335078534034\n",
            "<====================================================================================>\n",
            "Iteration: 145. || Loss: 0.0018192840507254004. ||  Accuracy: 37.95811518324607\n",
            "<====================================================================================>\n",
            "Iteration: 146. || Loss: 0.0018192840507254004. ||  Accuracy: 38.21989528795812\n",
            "<====================================================================================>\n",
            "Iteration: 147. || Loss: 0.0018192840507254004. ||  Accuracy: 38.481675392670155\n",
            "<====================================================================================>\n",
            "Iteration: 148. || Loss: 0.0018192840507254004. ||  Accuracy: 38.7434554973822\n",
            "<====================================================================================>\n",
            "Iteration: 149. || Loss: 0.0018192840507254004. ||  Accuracy: 39.00523560209424\n",
            "<====================================================================================>\n",
            "Iteration: 150. || Loss: 0.0018192840507254004. ||  Accuracy: 39.26701570680628\n",
            "<====================================================================================>\n",
            "Iteration: 151. || Loss: 0.0018192840507254004. ||  Accuracy: 39.52879581151832\n",
            "<====================================================================================>\n",
            "Iteration: 152. || Loss: 0.0018192840507254004. ||  Accuracy: 39.79057591623037\n",
            "<====================================================================================>\n",
            "Iteration: 153. || Loss: 0.0018192840507254004. ||  Accuracy: 40.05235602094241\n",
            "<====================================================================================>\n",
            "Iteration: 154. || Loss: 0.0018192840507254004. ||  Accuracy: 40.31413612565445\n",
            "<====================================================================================>\n",
            "Iteration: 155. || Loss: 0.0018192840507254004. ||  Accuracy: 40.575916230366495\n",
            "<====================================================================================>\n",
            "Iteration: 156. || Loss: 0.0018192840507254004. ||  Accuracy: 40.83769633507853\n",
            "<====================================================================================>\n",
            "Iteration: 157. || Loss: 0.0018192840507254004. ||  Accuracy: 41.09947643979058\n",
            "<====================================================================================>\n",
            "Iteration: 158. || Loss: 0.0018192840507254004. ||  Accuracy: 41.361256544502616\n",
            "<====================================================================================>\n",
            "Iteration: 159. || Loss: 0.0018192840507254004. ||  Accuracy: 41.62303664921466\n",
            "<====================================================================================>\n",
            "Iteration: 160. || Loss: 0.0018192840507254004. ||  Accuracy: 41.8848167539267\n",
            "<====================================================================================>\n",
            "Iteration: 161. || Loss: 0.0018192840507254004. ||  Accuracy: 42.146596858638745\n",
            "<====================================================================================>\n",
            "Iteration: 162. || Loss: 0.0018192840507254004. ||  Accuracy: 42.40837696335078\n",
            "<====================================================================================>\n",
            "Iteration: 163. || Loss: 0.0018192840507254004. ||  Accuracy: 42.67015706806283\n",
            "<====================================================================================>\n",
            "Iteration: 164. || Loss: 0.0018192840507254004. ||  Accuracy: 42.931937172774866\n",
            "<====================================================================================>\n",
            "Iteration: 165. || Loss: 0.0018192840507254004. ||  Accuracy: 43.19371727748691\n",
            "<====================================================================================>\n",
            "Iteration: 166. || Loss: 0.0018192840507254004. ||  Accuracy: 43.455497382198956\n",
            "<====================================================================================>\n",
            "Iteration: 167. || Loss: 0.0018192840507254004. ||  Accuracy: 43.717277486910994\n",
            "<====================================================================================>\n",
            "Iteration: 168. || Loss: 0.0018192840507254004. ||  Accuracy: 43.97905759162304\n",
            "<====================================================================================>\n",
            "Iteration: 169. || Loss: 0.0018192840507254004. ||  Accuracy: 44.24083769633508\n",
            "<====================================================================================>\n",
            "Iteration: 170. || Loss: 0.0018192840507254004. ||  Accuracy: 44.50261780104712\n",
            "<====================================================================================>\n",
            "Iteration: 171. || Loss: 0.0018192840507254004. ||  Accuracy: 44.76439790575916\n",
            "<====================================================================================>\n",
            "Iteration: 172. || Loss: 0.0018192840507254004. ||  Accuracy: 45.026178010471206\n",
            "<====================================================================================>\n",
            "Iteration: 173. || Loss: 0.0018192840507254004. ||  Accuracy: 45.287958115183244\n",
            "<====================================================================================>\n",
            "Iteration: 174. || Loss: 0.0018192840507254004. ||  Accuracy: 45.54973821989529\n",
            "<====================================================================================>\n",
            "Iteration: 175. || Loss: 0.0018192840507254004. ||  Accuracy: 45.81151832460733\n",
            "<====================================================================================>\n",
            "Iteration: 176. || Loss: 0.0018192840507254004. ||  Accuracy: 46.07329842931937\n",
            "<====================================================================================>\n",
            "Iteration: 177. || Loss: 0.0018192840507254004. ||  Accuracy: 46.33507853403141\n",
            "<====================================================================================>\n",
            "Iteration: 178. || Loss: 0.0018192840507254004. ||  Accuracy: 46.596858638743456\n",
            "<====================================================================================>\n",
            "Iteration: 179. || Loss: 0.0018192840507254004. ||  Accuracy: 46.8586387434555\n",
            "<====================================================================================>\n",
            "Iteration: 180. || Loss: 0.0018192840507254004. ||  Accuracy: 47.12041884816754\n",
            "<====================================================================================>\n",
            "Iteration: 181. || Loss: 0.0018192840507254004. ||  Accuracy: 47.382198952879584\n",
            "<====================================================================================>\n",
            "Iteration: 182. || Loss: 0.0018192840507254004. ||  Accuracy: 47.64397905759162\n",
            "<====================================================================================>\n",
            "Iteration: 183. || Loss: 0.0018192840507254004. ||  Accuracy: 47.90575916230367\n",
            "<====================================================================================>\n",
            "Iteration: 184. || Loss: 0.0018192840507254004. ||  Accuracy: 48.167539267015705\n",
            "<====================================================================================>\n",
            "Iteration: 185. || Loss: 0.0018192840507254004. ||  Accuracy: 48.42931937172775\n",
            "<====================================================================================>\n",
            "Iteration: 186. || Loss: 0.0018192840507254004. ||  Accuracy: 48.69109947643979\n",
            "<====================================================================================>\n",
            "Iteration: 187. || Loss: 0.0018192840507254004. ||  Accuracy: 48.952879581151834\n",
            "<====================================================================================>\n",
            "Iteration: 188. || Loss: 0.0018192840507254004. ||  Accuracy: 49.21465968586387\n",
            "<====================================================================================>\n",
            "Iteration: 189. || Loss: 0.0018192840507254004. ||  Accuracy: 49.47643979057592\n",
            "<====================================================================================>\n",
            "Iteration: 190. || Loss: 0.0018192840507254004. ||  Accuracy: 49.738219895287955\n",
            "<====================================================================================>\n",
            "Iteration: 191. || Loss: 0.0018192840507254004. ||  Accuracy: 50.0\n",
            "<====================================================================================>\n",
            "Iteration: 192. || Loss: 0.0018192840507254004. ||  Accuracy: 50.261780104712045\n",
            "<====================================================================================>\n",
            "Iteration: 193. || Loss: 0.0018192840507254004. ||  Accuracy: 50.52356020942408\n",
            "<====================================================================================>\n",
            "Iteration: 194. || Loss: 0.0018192840507254004. ||  Accuracy: 50.78534031413613\n",
            "<====================================================================================>\n",
            "Iteration: 195. || Loss: 0.0018192840507254004. ||  Accuracy: 51.047120418848166\n",
            "<====================================================================================>\n",
            "Iteration: 196. || Loss: 0.0018192840507254004. ||  Accuracy: 51.30890052356021\n",
            "<====================================================================================>\n",
            "Iteration: 197. || Loss: 0.0018192840507254004. ||  Accuracy: 51.57068062827225\n",
            "<====================================================================================>\n",
            "Iteration: 198. || Loss: 0.0018192840507254004. ||  Accuracy: 51.832460732984295\n",
            "<====================================================================================>\n",
            "Iteration: 199. || Loss: 0.0018192840507254004. ||  Accuracy: 52.09424083769633\n",
            "<====================================================================================>\n",
            "Iteration: 200. || Loss: 0.0018192840507254004. ||  Accuracy: 52.35602094240838\n",
            "<====================================================================================>\n",
            "Iteration: 201. || Loss: 0.0018192840507254004. ||  Accuracy: 52.617801047120416\n",
            "<====================================================================================>\n",
            "Iteration: 202. || Loss: 0.0018192840507254004. ||  Accuracy: 52.87958115183246\n",
            "<====================================================================================>\n",
            "Iteration: 203. || Loss: 0.0018192840507254004. ||  Accuracy: 53.1413612565445\n",
            "<====================================================================================>\n",
            "Iteration: 204. || Loss: 0.0018192840507254004. ||  Accuracy: 53.403141361256544\n",
            "<====================================================================================>\n",
            "Iteration: 205. || Loss: 0.0018192840507254004. ||  Accuracy: 53.66492146596859\n",
            "<====================================================================================>\n",
            "Iteration: 206. || Loss: 0.0018192840507254004. ||  Accuracy: 53.92670157068063\n",
            "<====================================================================================>\n",
            "Iteration: 207. || Loss: 0.0018192840507254004. ||  Accuracy: 54.18848167539267\n",
            "<====================================================================================>\n",
            "Iteration: 208. || Loss: 0.0018192840507254004. ||  Accuracy: 54.45026178010471\n",
            "<====================================================================================>\n",
            "Iteration: 209. || Loss: 0.0018192840507254004. ||  Accuracy: 54.712041884816756\n",
            "<====================================================================================>\n",
            "Iteration: 210. || Loss: 0.0018192840507254004. ||  Accuracy: 54.973821989528794\n",
            "<====================================================================================>\n",
            "Iteration: 211. || Loss: 0.0018192840507254004. ||  Accuracy: 55.23560209424084\n",
            "<====================================================================================>\n",
            "Iteration: 212. || Loss: 0.0018192840507254004. ||  Accuracy: 55.49738219895288\n",
            "<====================================================================================>\n",
            "Iteration: 213. || Loss: 0.0018192840507254004. ||  Accuracy: 55.75916230366492\n",
            "<====================================================================================>\n",
            "Iteration: 214. || Loss: 0.0018192840507254004. ||  Accuracy: 56.02094240837696\n",
            "<====================================================================================>\n",
            "Iteration: 215. || Loss: 0.0018192840507254004. ||  Accuracy: 56.282722513089006\n",
            "<====================================================================================>\n",
            "Iteration: 216. || Loss: 0.0018192840507254004. ||  Accuracy: 56.544502617801044\n",
            "<====================================================================================>\n",
            "Iteration: 217. || Loss: 0.0018192840507254004. ||  Accuracy: 56.80628272251309\n",
            "<====================================================================================>\n",
            "Iteration: 218. || Loss: 0.0018192840507254004. ||  Accuracy: 57.068062827225134\n",
            "<====================================================================================>\n",
            "Iteration: 219. || Loss: 0.0018192840507254004. ||  Accuracy: 57.32984293193717\n",
            "<====================================================================================>\n",
            "Iteration: 220. || Loss: 0.0018192840507254004. ||  Accuracy: 57.59162303664922\n",
            "<====================================================================================>\n",
            "Iteration: 221. || Loss: 0.0018192840507254004. ||  Accuracy: 57.853403141361255\n",
            "<====================================================================================>\n",
            "Iteration: 222. || Loss: 0.0018192840507254004. ||  Accuracy: 58.1151832460733\n",
            "<====================================================================================>\n",
            "Iteration: 223. || Loss: 0.0018192840507254004. ||  Accuracy: 58.37696335078534\n",
            "<====================================================================================>\n",
            "Iteration: 224. || Loss: 0.0018192840507254004. ||  Accuracy: 58.638743455497384\n",
            "<====================================================================================>\n",
            "Iteration: 225. || Loss: 0.0018192840507254004. ||  Accuracy: 58.90052356020942\n",
            "<====================================================================================>\n",
            "Iteration: 226. || Loss: 0.0018192840507254004. ||  Accuracy: 59.16230366492147\n",
            "<====================================================================================>\n",
            "Iteration: 227. || Loss: 0.0018192840507254004. ||  Accuracy: 59.424083769633505\n",
            "<====================================================================================>\n",
            "Iteration: 228. || Loss: 0.0018192840507254004. ||  Accuracy: 59.68586387434555\n",
            "<====================================================================================>\n",
            "Iteration: 229. || Loss: 0.0018192840507254004. ||  Accuracy: 59.94764397905759\n",
            "<====================================================================================>\n",
            "Iteration: 230. || Loss: 0.0018192840507254004. ||  Accuracy: 60.20942408376963\n",
            "<====================================================================================>\n",
            "Iteration: 231. || Loss: 0.0018192840507254004. ||  Accuracy: 60.47120418848168\n",
            "<====================================================================================>\n",
            "Iteration: 232. || Loss: 0.0018192840507254004. ||  Accuracy: 60.73298429319372\n",
            "<====================================================================================>\n",
            "Iteration: 233. || Loss: 0.0018192840507254004. ||  Accuracy: 60.99476439790576\n",
            "<====================================================================================>\n",
            "Iteration: 234. || Loss: 0.0018192840507254004. ||  Accuracy: 61.2565445026178\n",
            "<====================================================================================>\n",
            "Iteration: 235. || Loss: 0.0018192840507254004. ||  Accuracy: 61.2565445026178\n",
            "<====================================================================================>\n",
            "Iteration: 236. || Loss: 0.0018192840507254004. ||  Accuracy: 61.2565445026178\n",
            "<====================================================================================>\n",
            "Iteration: 237. || Loss: 0.0018192840507254004. ||  Accuracy: 61.2565445026178\n",
            "<====================================================================================>\n",
            "Iteration: 238. || Loss: 0.0018192840507254004. ||  Accuracy: 61.2565445026178\n",
            "<====================================================================================>\n",
            "Iteration: 239. || Loss: 0.0018192840507254004. ||  Accuracy: 61.2565445026178\n",
            "<====================================================================================>\n",
            "Iteration: 240. || Loss: 0.0018192840507254004. ||  Accuracy: 61.2565445026178\n",
            "<====================================================================================>\n"
          ]
        }
      ]
    }
  ]
}